model:
  arch: grandtour_tf
  model_type: pretrain_llama_v2
  freeze_vit: True
  freeze_qformer: True
  max_txt_len: 2048
  end_sym: "</s>"
  low_resource: False

  frozen_llama_proj: False
  frozen_video_Qformer: False

  # QFormer
  num_query_token: 32

  vit_model: "ckpt/eva-vit-g/eva_vit_g.pth"
  llama_model: "ckpt/Video-LLaMA-2-7B-Finetuned/llama-2-7b-chat-hf/"
  q_former_model: "ckpt/instruct-blip/instruct_blip_vicuna7b_trimmed.pth"
  ckpt: "ckpt/timechat/TimeChat-7b/timechat_7b.pth"

  fusion_head_layers: 2
  max_frame_pos: 96
  fusion_header_type: "seqTransf"

  use_grad_checkpoint: True
  lora: True
  lora_r: 64
  lora_alpha: 64
  lora_inference_mode: False
  qformer_text_input: False
  window_size: 4
  stride: 4

  is_tf: True
  image_size: [512, 288]
  
data:
  vis_root: "/scratch/users/atacelen/Reconstructions3D"
  ann_root: "/scratch/users/atacelen/Reconstructions3D/annotations_true_false.json"
  num_video_query_token: 32
  tokenizer_name: "ckpt/Video-LLaMA-2-7B-Finetuned/llama-2-7b-chat-hf/"
  data_type: video
  model_type: "llama_v2"
  sample_type: 'rand'
  max_txt_len: 1536
  stride: 4

train:
  batch_size: 1
  val_ratio: 0.2
  num_workers: 1
  weight_decay: 0.001
  init_lr: 4e-5
  min_lr: 1e-5
  warmup_lr: 1e-5
  use_amp: True
  # iters_per_epoch: #TODO
  log_freq: 50
  accum_grad_iters: 8
  snapshot_path: "ckpt/snapshot.pth"


run:
  output_dir: "/scratch/users/atacelen/housetour/results/"
  max_epochs: 1
  eval_only: False